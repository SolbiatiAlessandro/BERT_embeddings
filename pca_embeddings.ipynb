{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from bert_serving.client import BertClient\n",
    "from bert_serving.server import BertServer\n",
    "from bert_serving.server.helper import get_args_parser\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_sentences = ['Music is an art form, and cultural activity, whose medium is sound', 'In its most general form, the activities describing music as an art form or cultural activity include the creation of works of music (songs, tunes, symphonies, and so on), the criticism of music, the study of the history of music, and the aesthetic examination of music.', 'There are many types of music, including popular music, traditional music, art music, music written for religious ceremonies and work songs such as chanteys', 'Music can be divided into genres (e.g., country music) and genres can be further divided into subgenres (e.g., country blues and pop country are two of the many country subgenres), although the dividing lines and relationships between music genres are often subtle, sometimes open to personal interpretation, and occasionally controversial.']\n",
    "computer_sentences =  ['A computer is a machine that can be instructed to carry ouAt sequences of arithmetic or logical operations automatically via computer programming.', 'The first digital electronic calculating machines were developed during World War II. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit (IC) chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s.' 'Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen).']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = music_sentences + computer_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_layer = 1\n",
    "subset_vec_all_layers = []\n",
    "port = 6006\n",
    "port_out = 6007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = [\n",
    "    '-model_dir', '../uncased_L-12_H-768_A-12/',\n",
    "    '-num_worker', '2',\n",
    "    '-port', str(port),\n",
    "    '-port_out', str(port_out),\n",
    "    '-max_seq_len', '20',\n",
    "    # '-client_batch_size', '2048',\n",
    "    '-max_batch_size', '256',\n",
    "    # '-num_client', '1',\n",
    "    '-pooling_strategy', 'REDUCE_MEAN',\n",
    "    '-pooling_layer', '-2',\n",
    "    '-gpu_memory_fraction', '0.2',\n",
    "    '-device','3',\n",
    "]\n",
    "args = get_args_parser().parse_args(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: ../uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: ../uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpcq2wj9vf\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpcq2wj9vf\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait until server is ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "W:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:246]:no GPU available, fall back to CPU\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> cpu\n",
      "\t\tworker  1 -> cpu\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpcq2wj9vf\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpcq2wj9vf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:164]:all set, ready to serve request!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:180]:new config request\treq id: 1\tclient: b'91618f5d-aeaf-4524-9abe-ed620e2062f3'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:348]:send config\tclient b'91618f5d-aeaf-4524-9abe-ed620e2062f3'\n",
      "/Users/lessandro/Coding/AI/BERT/embeddings/venv/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=20\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:196]:new encode request\treq id: 2\tsize: 6\tclient: b'91618f5d-aeaf-4524-9abe-ed620e2062f3'\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:568]:new job\tsocket: 0\tsize: 6\tclient: b'91618f5d-aeaf-4524-9abe-ed620e2062f3#2'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:342]:job register\tsize: 6\tjob id: b'91618f5d-aeaf-4524-9abe-ed620e2062f3#2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server config:\n",
      "                        client\t=\t91618f5d-aeaf-4524-9abe-ed620e2062f3\n",
      "                   num_process\t=\t3                             \n",
      "          ventilator -> worker\t=\t['ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp4KqYuf/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpuyR9Ec/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp2FC6nk/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpOj0ORk/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpkPGF9p/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpnTwg8r/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp4TP78g/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpaJULrq/socket']\n",
      "                worker -> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpN0WFnf/socket\n",
      "           ventilator <-> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp2nJCQe/socket\n",
      "           server_current_time\t=\t2020-09-24 14:38:25.760683    \n",
      "                    device_map\t=\t[3]                           \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.2                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t20                            \n",
      "                     model_dir\t=\t../uncased_L-12_H-768_A-12/   \n",
      "        no_position_embeddings\t=\tFalse                         \n",
      "              no_special_token\t=\tFalse                         \n",
      "                    num_worker\t=\t2                             \n",
      "                 pooling_layer\t=\t[-1]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t6006                          \n",
      "                      port_out\t=\t6007                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.7.5 (default, Oct 22 2019, 10:35:10) \n",
      "[Clang 10.0.1 (clang-1001.0.46.4)]\n",
      "                server_version\t=\t1.10.0                        \n",
      "                 pyzmq_version\t=\t19.0.2                        \n",
      "                   zmq_version\t=\t4.3.2                         \n",
      "             server_start_time\t=\t2020-09-24 14:37:55.435125    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:543]:job done\tsize: (6, 768)\tclient: b'91618f5d-aeaf-4524-9abe-ed620e2062f3#2'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:334]:collect b'EMBEDDINGS' b'91618f5d-aeaf-4524-9abe-ed620e2062f3#2' (E:6/T:0/A:6)\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:357]:send back\tsize: 6\tjob id: b'91618f5d-aeaf-4524-9abe-ed620e2062f3#2'\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:clo: 89]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:274]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:279]:terminated!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:219]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done at layer -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: ../uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: ../uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpfqculwg0\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpfqculwg0\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait until server is ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "W:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:246]:no GPU available, fall back to CPU\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> cpu\n",
      "\t\tworker  1 -> cpu\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpfqculwg0\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpfqculwg0\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:164]:all set, ready to serve request!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:180]:new config request\treq id: 1\tclient: b'0dded245-3fcb-414d-9445-092faa92d4fc'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:348]:send config\tclient b'0dded245-3fcb-414d-9445-092faa92d4fc'\n",
      "/Users/lessandro/Coding/AI/BERT/embeddings/venv/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=20\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:196]:new encode request\treq id: 2\tsize: 6\tclient: b'0dded245-3fcb-414d-9445-092faa92d4fc'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:342]:job register\tsize: 6\tjob id: b'0dded245-3fcb-414d-9445-092faa92d4fc#2'\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:568]:new job\tsocket: 0\tsize: 6\tclient: b'0dded245-3fcb-414d-9445-092faa92d4fc#2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding...\n",
      "server config:\n",
      "                        client\t=\t0dded245-3fcb-414d-9445-092faa92d4fc\n",
      "                   num_process\t=\t3                             \n",
      "          ventilator -> worker\t=\t['ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpANoGjP/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp7oKcCI/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpNsstVX/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpvXBNO6/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpt9etAR/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpwaUesm/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpRHpXIq/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpeZ7LGB/socket']\n",
      "                worker -> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpcc9zuT/socket\n",
      "           ventilator <-> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpKYq7MC/socket\n",
      "           server_current_time\t=\t2020-09-24 14:38:52.969214    \n",
      "                    device_map\t=\t[3]                           \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.2                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t20                            \n",
      "                     model_dir\t=\t../uncased_L-12_H-768_A-12/   \n",
      "        no_position_embeddings\t=\tFalse                         \n",
      "              no_special_token\t=\tFalse                         \n",
      "                    num_worker\t=\t2                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t6006                          \n",
      "                      port_out\t=\t6007                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.7.5 (default, Oct 22 2019, 10:35:10) \n",
      "[Clang 10.0.1 (clang-1001.0.46.4)]\n",
      "                server_version\t=\t1.10.0                        \n",
      "                 pyzmq_version\t=\t19.0.2                        \n",
      "                   zmq_version\t=\t4.3.2                         \n",
      "             server_start_time\t=\t2020-09-24 14:38:26.670842    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:543]:job done\tsize: (6, 768)\tclient: b'0dded245-3fcb-414d-9445-092faa92d4fc#2'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:334]:collect b'EMBEDDINGS' b'0dded245-3fcb-414d-9445-092faa92d4fc#2' (E:6/T:0/A:6)\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:357]:send back\tsize: 6\tjob id: b'0dded245-3fcb-414d-9445-092faa92d4fc#2'\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:clo: 89]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:274]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:279]:terminated!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:219]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done at layer -2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: ../uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: ../uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp0t44_9su\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp0t44_9su\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait until server is ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "W:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:246]:no GPU available, fall back to CPU\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> cpu\n",
      "\t\tworker  1 -> cpu\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp0t44_9su\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp0t44_9su\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:164]:all set, ready to serve request!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:180]:new config request\treq id: 1\tclient: b'68104a91-9ffa-4e3c-b785-88c3e2d18424'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:348]:send config\tclient b'68104a91-9ffa-4e3c-b785-88c3e2d18424'\n",
      "/Users/lessandro/Coding/AI/BERT/embeddings/venv/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=20\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:196]:new encode request\treq id: 2\tsize: 6\tclient: b'68104a91-9ffa-4e3c-b785-88c3e2d18424'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:342]:job register\tsize: 6\tjob id: b'68104a91-9ffa-4e3c-b785-88c3e2d18424#2'\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:568]:new job\tsocket: 0\tsize: 6\tclient: b'68104a91-9ffa-4e3c-b785-88c3e2d18424#2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding...\n",
      "server config:\n",
      "                        client\t=\t68104a91-9ffa-4e3c-b785-88c3e2d18424\n",
      "                   num_process\t=\t3                             \n",
      "          ventilator -> worker\t=\t['ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpL1ozSL/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpzRcArP/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpLQhhVg/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpRoaiQf/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp6xvRCJ/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpyPcfvY/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpoDrWMK/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp58ww5Y/socket']\n",
      "                worker -> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpUOP8IQ/socket\n",
      "           ventilator <-> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpnqWmLB/socket\n",
      "           server_current_time\t=\t2020-09-24 14:39:19.742429    \n",
      "                    device_map\t=\t[3]                           \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.2                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t20                            \n",
      "                     model_dir\t=\t../uncased_L-12_H-768_A-12/   \n",
      "        no_position_embeddings\t=\tFalse                         \n",
      "              no_special_token\t=\tFalse                         \n",
      "                    num_worker\t=\t2                             \n",
      "                 pooling_layer\t=\t[-3]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t6006                          \n",
      "                      port_out\t=\t6007                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.7.5 (default, Oct 22 2019, 10:35:10) \n",
      "[Clang 10.0.1 (clang-1001.0.46.4)]\n",
      "                server_version\t=\t1.10.0                        \n",
      "                 pyzmq_version\t=\t19.0.2                        \n",
      "                   zmq_version\t=\t4.3.2                         \n",
      "             server_start_time\t=\t2020-09-24 14:38:53.643150    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:543]:job done\tsize: (6, 768)\tclient: b'68104a91-9ffa-4e3c-b785-88c3e2d18424#2'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:334]:collect b'EMBEDDINGS' b'68104a91-9ffa-4e3c-b785-88c3e2d18424#2' (E:6/T:0/A:6)\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:357]:send back\tsize: 6\tjob id: b'68104a91-9ffa-4e3c-b785-88c3e2d18424#2'\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:clo: 89]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:274]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:279]:terminated!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:219]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done at layer -3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: ../uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: ../uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp8m229k8q\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp8m229k8q\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait until server is ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "W:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:246]:no GPU available, fall back to CPU\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> cpu\n",
      "\t\tworker  1 -> cpu\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp8m229k8q\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp8m229k8q\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:164]:all set, ready to serve request!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:180]:new config request\treq id: 1\tclient: b'459e93e8-2ab6-4d94-9f9f-cb6ec9ad4ad6'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:348]:send config\tclient b'459e93e8-2ab6-4d94-9f9f-cb6ec9ad4ad6'\n",
      "/Users/lessandro/Coding/AI/BERT/embeddings/venv/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=20\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:196]:new encode request\treq id: 2\tsize: 6\tclient: b'459e93e8-2ab6-4d94-9f9f-cb6ec9ad4ad6'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:342]:job register\tsize: 6\tjob id: b'459e93e8-2ab6-4d94-9f9f-cb6ec9ad4ad6#2'\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:568]:new job\tsocket: 0\tsize: 6\tclient: b'459e93e8-2ab6-4d94-9f9f-cb6ec9ad4ad6#2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding...\n",
      "server config:\n",
      "                        client\t=\t459e93e8-2ab6-4d94-9f9f-cb6ec9ad4ad6\n",
      "                   num_process\t=\t3                             \n",
      "          ventilator -> worker\t=\t['ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpFW6kpR/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpIpnr12/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpG8XklT/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp3R32cT/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpIfyfbU/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpgfJwlf/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpGlbVYY/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp5qpqoh/socket']\n",
      "                worker -> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmprzzJi7/socket\n",
      "           ventilator <-> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpBmU5rb/socket\n",
      "           server_current_time\t=\t2020-09-24 14:39:47.233134    \n",
      "                    device_map\t=\t[3]                           \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.2                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t20                            \n",
      "                     model_dir\t=\t../uncased_L-12_H-768_A-12/   \n",
      "        no_position_embeddings\t=\tFalse                         \n",
      "              no_special_token\t=\tFalse                         \n",
      "                    num_worker\t=\t2                             \n",
      "                 pooling_layer\t=\t[-4]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t6006                          \n",
      "                      port_out\t=\t6007                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.7.5 (default, Oct 22 2019, 10:35:10) \n",
      "[Clang 10.0.1 (clang-1001.0.46.4)]\n",
      "                server_version\t=\t1.10.0                        \n",
      "                 pyzmq_version\t=\t19.0.2                        \n",
      "                   zmq_version\t=\t4.3.2                         \n",
      "             server_start_time\t=\t2020-09-24 14:39:20.434488    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:543]:job done\tsize: (6, 768)\tclient: b'459e93e8-2ab6-4d94-9f9f-cb6ec9ad4ad6#2'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:334]:collect b'EMBEDDINGS' b'459e93e8-2ab6-4d94-9f9f-cb6ec9ad4ad6#2' (E:6/T:0/A:6)\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:357]:send back\tsize: 6\tjob id: b'459e93e8-2ab6-4d94-9f9f-cb6ec9ad4ad6#2'\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:clo: 89]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:274]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:279]:terminated!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:219]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done at layer -4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: ../uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: ../uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpx7sujbhd\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpx7sujbhd\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait until server is ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "W:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:246]:no GPU available, fall back to CPU\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> cpu\n",
      "\t\tworker  1 -> cpu\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpx7sujbhd\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpx7sujbhd\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:164]:all set, ready to serve request!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:180]:new config request\treq id: 1\tclient: b'5ce79d8d-352f-4518-8acf-2999605ba6a5'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:348]:send config\tclient b'5ce79d8d-352f-4518-8acf-2999605ba6a5'\n",
      "/Users/lessandro/Coding/AI/BERT/embeddings/venv/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=20\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:196]:new encode request\treq id: 2\tsize: 6\tclient: b'5ce79d8d-352f-4518-8acf-2999605ba6a5'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:342]:job register\tsize: 6\tjob id: b'5ce79d8d-352f-4518-8acf-2999605ba6a5#2'\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:568]:new job\tsocket: 0\tsize: 6\tclient: b'5ce79d8d-352f-4518-8acf-2999605ba6a5#2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding...\n",
      "server config:\n",
      "                        client\t=\t5ce79d8d-352f-4518-8acf-2999605ba6a5\n",
      "                   num_process\t=\t3                             \n",
      "          ventilator -> worker\t=\t['ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp3LBvA5/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpclEcFM/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp3QzCNI/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpHVNoDh/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpG1YSYL/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpSxzrJ5/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpuV1Jkl/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpEwrFvx/socket']\n",
      "                worker -> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpF1ffhJ/socket\n",
      "           ventilator <-> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpjuht4i/socket\n",
      "           server_current_time\t=\t2020-09-24 14:40:14.615328    \n",
      "                    device_map\t=\t[3]                           \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.2                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t20                            \n",
      "                     model_dir\t=\t../uncased_L-12_H-768_A-12/   \n",
      "        no_position_embeddings\t=\tFalse                         \n",
      "              no_special_token\t=\tFalse                         \n",
      "                    num_worker\t=\t2                             \n",
      "                 pooling_layer\t=\t[-5]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t6006                          \n",
      "                      port_out\t=\t6007                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.7.5 (default, Oct 22 2019, 10:35:10) \n",
      "[Clang 10.0.1 (clang-1001.0.46.4)]\n",
      "                server_version\t=\t1.10.0                        \n",
      "                 pyzmq_version\t=\t19.0.2                        \n",
      "                   zmq_version\t=\t4.3.2                         \n",
      "             server_start_time\t=\t2020-09-24 14:39:47.912161    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:543]:job done\tsize: (6, 768)\tclient: b'5ce79d8d-352f-4518-8acf-2999605ba6a5#2'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:334]:collect b'EMBEDDINGS' b'5ce79d8d-352f-4518-8acf-2999605ba6a5#2' (E:6/T:0/A:6)\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:357]:send back\tsize: 6\tjob id: b'5ce79d8d-352f-4518-8acf-2999605ba6a5#2'\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:clo: 89]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:274]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:279]:terminated!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:219]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done at layer -5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: ../uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: ../uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpp9kah6oi\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpp9kah6oi\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait until server is ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "W:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:246]:no GPU available, fall back to CPU\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> cpu\n",
      "\t\tworker  1 -> cpu\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpp9kah6oi\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpp9kah6oi\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:164]:all set, ready to serve request!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:180]:new config request\treq id: 1\tclient: b'f2f4aaf9-3702-45ec-a054-5781551d6c10'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:348]:send config\tclient b'f2f4aaf9-3702-45ec-a054-5781551d6c10'\n",
      "/Users/lessandro/Coding/AI/BERT/embeddings/venv/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=20\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:196]:new encode request\treq id: 2\tsize: 6\tclient: b'f2f4aaf9-3702-45ec-a054-5781551d6c10'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:342]:job register\tsize: 6\tjob id: b'f2f4aaf9-3702-45ec-a054-5781551d6c10#2'\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:568]:new job\tsocket: 0\tsize: 6\tclient: b'f2f4aaf9-3702-45ec-a054-5781551d6c10#2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding...\n",
      "server config:\n",
      "                        client\t=\tf2f4aaf9-3702-45ec-a054-5781551d6c10\n",
      "                   num_process\t=\t3                             \n",
      "          ventilator -> worker\t=\t['ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpI7W9TK/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpZQ9XAL/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpW0teJn/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpuOuS8U/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpYomoyx/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpS4GXv7/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpo3MuTG/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmphkEKXS/socket']\n",
      "                worker -> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpl9mXC6/socket\n",
      "           ventilator <-> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpihcLhg/socket\n",
      "           server_current_time\t=\t2020-09-24 14:40:42.169475    \n",
      "                    device_map\t=\t[3]                           \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.2                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t20                            \n",
      "                     model_dir\t=\t../uncased_L-12_H-768_A-12/   \n",
      "        no_position_embeddings\t=\tFalse                         \n",
      "              no_special_token\t=\tFalse                         \n",
      "                    num_worker\t=\t2                             \n",
      "                 pooling_layer\t=\t[-6]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t6006                          \n",
      "                      port_out\t=\t6007                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.7.5 (default, Oct 22 2019, 10:35:10) \n",
      "[Clang 10.0.1 (clang-1001.0.46.4)]\n",
      "                server_version\t=\t1.10.0                        \n",
      "                 pyzmq_version\t=\t19.0.2                        \n",
      "                   zmq_version\t=\t4.3.2                         \n",
      "             server_start_time\t=\t2020-09-24 14:40:15.272825    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:543]:job done\tsize: (6, 768)\tclient: b'f2f4aaf9-3702-45ec-a054-5781551d6c10#2'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:334]:collect b'EMBEDDINGS' b'f2f4aaf9-3702-45ec-a054-5781551d6c10#2' (E:6/T:0/A:6)\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:357]:send back\tsize: 6\tjob id: b'f2f4aaf9-3702-45ec-a054-5781551d6c10#2'\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:clo: 89]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:274]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:279]:terminated!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:219]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done at layer -6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: ../uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: ../uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpejifvcqw\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpejifvcqw\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait until server is ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "W:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:246]:no GPU available, fall back to CPU\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> cpu\n",
      "\t\tworker  1 -> cpu\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpejifvcqw\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpejifvcqw\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:559]:ready and listening!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:164]:all set, ready to serve request!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:180]:new config request\treq id: 1\tclient: b'6261e291-1172-4a6f-b992-b695a4902b63'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:348]:send config\tclient b'6261e291-1172-4a6f-b992-b695a4902b63'\n",
      "/Users/lessandro/Coding/AI/BERT/embeddings/venv/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=20\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:196]:new encode request\treq id: 2\tsize: 6\tclient: b'6261e291-1172-4a6f-b992-b695a4902b63'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:342]:job register\tsize: 6\tjob id: b'6261e291-1172-4a6f-b992-b695a4902b63#2'\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:gen:568]:new job\tsocket: 0\tsize: 6\tclient: b'6261e291-1172-4a6f-b992-b695a4902b63#2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding...\n",
      "server config:\n",
      "                        client\t=\t6261e291-1172-4a6f-b992-b695a4902b63\n",
      "                   num_process\t=\t3                             \n",
      "          ventilator -> worker\t=\t['ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpPg9Ohx/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpwe1lAP/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpxZVEkL/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpx3rpKw/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpYeSzlZ/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpDLWmVa/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpWUhM1Y/socket', 'ipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmphmYTnp/socket']\n",
      "                worker -> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmp7ywy2k/socket\n",
      "           ventilator <-> sink\t=\tipc:///var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpg1oNTX/socket\n",
      "           server_current_time\t=\t2020-09-24 14:41:09.008820    \n",
      "                    device_map\t=\t[3]                           \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "                 do_lower_case\t=\tTrue                          \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.2                           \n",
      "                 graph_tmp_dir\t=\tNone                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t20                            \n",
      "                     model_dir\t=\t../uncased_L-12_H-768_A-12/   \n",
      "        no_position_embeddings\t=\tFalse                         \n",
      "              no_special_token\t=\tFalse                         \n",
      "                    num_worker\t=\t2                             \n",
      "                 pooling_layer\t=\t[-7]                          \n",
      "              pooling_strategy\t=\t2                             \n",
      "                          port\t=\t6006                          \n",
      "                      port_out\t=\t6007                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tFalse                         \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.7.5 (default, Oct 22 2019, 10:35:10) \n",
      "[Clang 10.0.1 (clang-1001.0.46.4)]\n",
      "                server_version\t=\t1.10.0                        \n",
      "                 pyzmq_version\t=\t19.0.2                        \n",
      "                   zmq_version\t=\t4.3.2                         \n",
      "             server_start_time\t=\t2020-09-24 14:40:42.854797    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:543]:job done\tsize: (6, 768)\tclient: b'6261e291-1172-4a6f-b992-b695a4902b63#2'\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:334]:collect b'EMBEDDINGS' b'6261e291-1172-4a6f-b992-b695a4902b63#2' (E:6/T:0/A:6)\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:357]:send back\tsize: 6\tjob id: b'6261e291-1172-4a6f-b992-b695a4902b63#2'\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:clo: 89]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:274]:shutting down...\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:clo:279]:terminated!\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:481]:shutting down...\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:clo:486]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:219]:terminated!\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 67]:freeze, optimize and export graph, could take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done at layer -7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: ../uncased_L-12_H-768_A-12/bert_config.json\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: ../uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
      "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpab5_w_nl\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 75]:optimized graph is stored at: /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpab5_w_nl\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:129]:bind all sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:133]:open 8 ventilator-worker sockets\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:136]:start the sink\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait until server is ready...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:222]:get devices\n",
      "I:\u001b[32mSINK\u001b[0m:[__i:_ru:306]:ready\n",
      "W:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:246]:no GPU available, fall back to CPU\n",
      "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:255]:device map: \n",
      "\t\tworker  0 -> cpu\n",
      "\t\tworker  1 -> cpu\n",
      "I:\u001b[33mWORKER-0\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpab5_w_nl\n",
      "I:\u001b[33mWORKER-1\u001b[0m:[__i:_ru:531]:use device cpu, load graph from /var/folders/tt/kktdtxb905n0tqr92f6dnn540000gn/T/tmpab5_w_nl\n"
     ]
    }
   ],
   "source": [
    "subset_vec_all_layers = []\n",
    "for pool_layer in range(1, 13):\n",
    "    setattr(args, 'pooling_layer', [-pool_layer])\n",
    "    server = BertServer(args)\n",
    "    server.start()\n",
    "    print('wait until server is ready...')\n",
    "    time.sleep(20)\n",
    "    print('encoding...')\n",
    "    bc = BertClient(port=port, port_out=port_out, show_server_config=True)\n",
    "    subset_vec_all_layers.append(bc.encode(sentences))\n",
    "    bc.close()\n",
    "    server.close()\n",
    "    print('done at layer -%d' % pool_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
